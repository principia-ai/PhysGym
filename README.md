<p align="center">
  <img src="physgym_logo.png" alt="PhysGym Logo" width="300"/>
</p>


<h3 align="center">Simulating Interactive Physics Discovery with Controlled Priors</h3>

<p align="center">
  <a href="https://arxiv.org/abs/2507.15550"><img src="https://img.shields.io/badge/arXiv-2507.15550-b31b1b.svg" alt="arXiv"></a>
  <a href="https://opensource.org/licenses/MIT"><img src="https://img.shields.io/badge/License-MIT-yellow.svg" alt="License: MIT"></a>
</p>

<p align="center">
A novel benchmark suite and simulation platform for rigorously assessing LLM-based scientific reasoning in interactive physics environments.
</p>

---

## ğŸ“¢ News

- [Sep 2025] ğŸ‰ PhysGym has been accepted to **NeurIPS 2025 Datasets and Benchmarks Track**! See you in San Diego.
- [Jul 2025] Paper released on [arXiv](https://arxiv.org/abs/2507.15550).

## Overview

PhysGym addresses a critical gap in evaluating AI scientific discovery capabilities: the inability to systematically control and vary the prior knowledge available to AI agents. Current benchmarks expose fixed sets of cues, making it impossible to distinguish between **memorization** and **true mechanistic inference**.

### The Core Challenge

Consider a pendulum experiment:
- With full context (variable names like "length", "gravity", descriptions), an agent can pattern-match to recover known solutions
- Without context (anonymized variables), the agent must actively probe the environment, design experiments, and form structural hypotheses

These scenarios involve identical physics but radically different cognitive demandsâ€”yet existing benchmarks cannot disentangle them.

### PhysGym's Solution

PhysGym provides **sophisticated control over prior knowledge levels**, allowing researchers to systematically investigate how LLM agents:
- Balance deductive reasoning (leveraging priors) with inductive learning (from experimentation)
- Adapt to unfamiliar problem settings
- Compensate for missing information through active exploration
- Construct and refine physics models from observations

## Key Features

- **Controlled Prior Knowledge (L1-L4)**: Systematically vary what contextual information agents receive, from full descriptions to completely anonymized variables
- **Interactive Physics Environments**: 97 curated physics problems from PHYBench, each with executable simulations
- **Realistic Constraints**: Limited experimental budgets (100 experiments) that mirror real scientific practice
- **Comprehensive Evaluation**:
  - **Success Rate**: Primary metric based on equation equivalence (symbolic + LLM-based)
  - **Consistency Metrics**: RÂ², MSE, Kendall's tau, MAPE for hypothesis-data alignment
  - **Task Difficulty Metrics**: Equation complexity, variable count analysis
- **Complete History Tracking**: Monitor exploration dynamics, hypothesis evolution, and reasoning strategies

## Prior Knowledge Levels

PhysGym's central innovation is fine-grained control over prior information. Four primary levels are provided:

| Level | Context | Variable Descriptions | Variable Names | Use Case |
|-------|---------|----------------------|----------------|----------|
| **L1** | âœ… Full problem description | âœ… Physical meanings | âœ… Conventional (e.g., `m`, `v`) | Reasoning with rich priors |
| **L2** | âŒ "Unknown context" | âœ… Physical meanings | âœ… Conventional | Partial information |
| **L3** | âŒ Hidden | âŒ Meaningless descriptions | âœ… Conventional | Minimal semantic cues |
| **L4** | âŒ Hidden | âŒ Meaningless descriptions | âŒ Anonymous (`var_1`, `var_2`) | Pure equation discovery |

**Key Findings from Baseline Experiments:**
- Performance declines with reduced priors (e.g., o4-mini: 62.89% â†’ 31% from L1 to L4)
- **Non-monotonic patterns**: Some problems solved only at lower prior levels, revealing conflicts between knowledge recall and discovery
- Prior knowledge becomes critical for high-dimensional problems (7+ variables)
- Reasoning-enabled models show better exploration dynamics under uncertainty

## Installation

### From Source (Development)

```bash
# Clone the repository
git clone https://github.com/principia-ai/PhysGym.git
cd PhysGym

# Install in editable mode
pip install -e .

# Or install with optional dependencies for local LLM support
pip install -e ".[local-llm]"

# Or install everything including dev tools
pip install -e ".[all]"
```

## Getting Started

### Configuration

1. Copy `api_keys.env.example` to `api_keys.env`
2. Add your API keys to `api_keys.env`:

### Quick Start: Basic Usage

```python
import physgym

# Create an experiment with a physics problem
experiment = physgym.ResearchInterface(
    env=285,           # Physics problem ID
    sample_quota=100,  # Maximum experiments
    mode="default"     # Prior level: "default" (L1), "no_context" (L2), 
                       # "no_description" (L3), "no_description_anonymous" (L4)
)

# Run experiments, test hypotheses, and evaluate
# See examples/basic_usage.py for complete walkthrough
```

For a complete example including experiment design, hypothesis testing, and metric evaluation, see [`examples/basic_usage.py`](examples/basic_usage.py).

For full benchmark evaluation with LLM agents, see [`experiments/run_baseline.py`](experiments/run_baseline.py) and the **Running Experiments** section below.

## Benchmark Components

### Dataset Structure

Each of the 97 physics problems is structured with:

```python
{
    "context": "Textual description of the physics problem",
    "solution": "Step-by-step derivation based on physics principles",
    "equation": "Ground-truth equation linking input/output quantities",
    "python_code": "Executable simulation with numerical validity checks",
    "input_variables": [
        {"name": "m", "description": "mass", "unit": "kg"},
        # ... more variables
    ],
    "output_variable": {"name": "v", "description": "velocity", "unit": "m/s"},
    "dummy_variables": ["Additional context variables not in core equation"]
}
```

### Architecture

```
PhysGym/
â”œâ”€â”€ physgym/              # ğŸ“¦ Core Benchmark Suite
â”‚   â”œâ”€â”€ interface.py      # Research interface with quota management & prior control
â”‚   â”œâ”€â”€ phyenv.py        # 97 executable physics environments
â”‚   â”œâ”€â”€ samples/         # Complete problem dataset with metadata
â”‚   â”œâ”€â”€ prompts/         # Evaluation & preprocessing prompts
â”‚   â””â”€â”€ utils/
â”‚       â”œâ”€â”€ metrics.py    # RÂ², MSE, Kendall's tau, symbolic equivalence
â”‚       â”œâ”€â”€ llm_providers.py  # Multi-provider LLM support
â”‚       â”œâ”€â”€ sandbox.py    # Safe execution environment
â”‚       â””â”€â”€ preprocess.py # Dataset construction tools
â”‚
â”œâ”€â”€ methods/              # ğŸ”¬ Baseline Agents
â”‚   â”œâ”€â”€ baseline_researcher.py  # Direct prompting baseline
â”‚   â”œâ”€â”€ react_agent.py          # ReAct-style agent
â”‚   â””â”€â”€ prompts/               # Agent-specific prompts
â”‚
â”œâ”€â”€ experiments/          # ğŸ§ª Benchmark Runners
â”‚   â””â”€â”€ run_baseline.py   # Full evaluation across all problems & prior levels
â”‚
â”œâ”€â”€ analysis/            # ğŸ“Š Result Analysis
â”‚   â””â”€â”€ postprocess.py   # Aggregate metrics, identify patterns
â”‚
â””â”€â”€ examples/            # ğŸ“š Usage Examples
    â””â”€â”€ basic_usage.py
```

### Key Components

**Environments (`phyenv.py`)**:
- Access to the 97 physics environments (in full_samples.json) converted from PHYBench
- Each accepts numerical inputs and returns physics-based outputs
- Input validation and domain constraints included

**Interface (`interface.py`)**:
- **Quota Management**: Enforce experimental budget (default: 100 experiments)
- **Prior Knowledge Control**: Configure L1-L4 information exposure
- **History Tracking**: Complete logs of interactions, hypotheses, evaluations
- **Automatic Evaluation**: Integrated metrics for real-time assessment

**Evaluation Metrics (`utils/metrics.py`)**:
- **Success Rate**: Primary metric (symbolic + LLM-based equivalence)
- **Consistency Metrics**: RÂ², MSE, Kendall's tau, MAPE

## Running Experiments

PhysGym provides several convenient scripts for running experiments. All scripts are located in the `scripts/` directory.

### Using Provided Scripts

**Quick Test (Single Problem)**
```bash
# Test on environment ID 285 with Gemini Flash
bash scripts/test.sh
```
Edit `scripts/test.sh` to customize the model, mode, and environment ID.

**Full Benchmark Evaluation (All 97 Problems)**
```bash
# Run complete benchmark with Gemini Pro
bash scripts/baseline.sh
```
Edit `scripts/baseline.sh` to change the model or mode.

**Local LLM with Ollama**
```bash
# Run with local Ollama model (e.g., gpt-oss:20b)
bash scripts/ollama_gpt.sh
```

**Local LLM Examples & Templates**
```bash
# View examples for running with Ollama, vLLM, etc.
bash scripts/run_local_llm_example.sh
```

### Direct Python Usage

**Single Problem**
```bash
python experiments/run_baseline.py \
    --env-id 285 \
    --mode default \
    --llm-model "google/gemini-2.5-flash" \
    --api-provider openrouter \
    --max-iterations 20 \
    --sample-quota 100
```

**All Problems (Batch)**
```bash
python experiments/run_baseline.py \
    --llm-model "google/gemini-2.5-flash" \
    --api-provider openrouter \
    --mode default \
    --idx-start 0 \
    --idx-end 97 \
    --sample-quota 100 \
    --max-iterations 20
```

### Prior Knowledge Modes

The `--mode` parameter controls the prior knowledge level:

- **`default`** â†’ Level 1 (L1): Full context + descriptions + conventional names
- **`no_context`** â†’ Level 2 (L2): No context, but descriptions + names available  
- **`no_description`** â†’ Level 3 (L3): No context/descriptions, conventional names only
- **`no_description_anonymous`** â†’ Level 4 (L4): Anonymized variables (var_1, var_2, etc.)

### Supported Models

PhysGym has been tested with:
- **OpenAI**: `openai/gpt-4`, `openai/o4-mini-high` (reasoning model)
- **Anthropic**: `anthropic/claude-3.7-sonnet`
- **Google**: `google/gemini-2.5-pro`, `google/gemini-2.5-flash`, `google/gemini-2.5-flash-preview:thinking`
- **DeepSeek**: `deepseek/deepseek-chat`, `deepseek/deepseek-r1`
- **Open Source** (via vLLM/Ollama): `Qwen3-32B`, `gpt-oss-20b`

For local models, use `--api-provider ollama` or `--api-provider vllm` with `--base-url` if needed.

## Scientific Discovery Workflow

PhysGym operationalizes the scientific discovery process:

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  1. Problem Setup                                       â”‚
â”‚     â€¢ Select physics environment (1-97)                 â”‚
â”‚     â€¢ Configure prior level (L1-L4)                     â”‚
â”‚     â€¢ Set experimental budget (default: 100)            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                   â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  2. Active Experimentation                              â”‚
â”‚     â€¢ Agent designs experiments (input valuations)      â”‚
â”‚     â€¢ Environment returns observations                  â”‚
â”‚     â€¢ Budget decrements with each experiment            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                   â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  3. Hypothesis Formation                                â”‚
â”‚     â€¢ Agent proposes equation based on observations     â”‚
â”‚     â€¢ Can request oracle test (symbolic equivalence)    â”‚
â”‚     â€¢ Receives consistency metrics (RÂ², MSE, etc.)      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                   â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  4. Iterative Refinement                                â”‚
â”‚     â€¢ Refine hypothesis based on feedback               â”‚
â”‚     â€¢ Design targeted experiments                       â”‚
â”‚     â€¢ Repeat until budget exhausted or solved           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                   â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  5. Evaluation                                          â”‚
â”‚     â€¢ Success: Symbolic equivalence to ground truth     â”‚
â”‚     â€¢ Metrics: RÂ², MSE, Kendall's tau, MAPE           â”‚
â”‚     â€¢ Analysis: Exploration dynamics, hypothesis count  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## Results Analysis

Process and analyze experiment results:

```bash
# Aggregate results and generate summary statistics
python analysis/postprocess.py

# Clean unsuccessful experiments (with remaining quotas)
python analysis/postprocess.py --clean
```

The script generates:
- `results_summary.json`: Aggregated metrics by model/configuration
- `models_data.json`: Detailed per-experiment data

For interactive exploration and visualization, refer to the notebook [`analysis/result_analysis.ipynb`](analysis/result_analysis.ipynb).

## Research Applications

PhysGym enables investigation of:

- **Curiosity-Driven Learning**: How agents explore to compress observations and improve world models
- **Prior vs. Posterior Balance**: When do agents rely on deduction vs. induction?
- **Experimental Design**: Can models design informative experiments or just random sampling?
- **Causal Reasoning**: Do models understand causal relationships or just correlations?
- **Generalization**: How do agents perform on unfamiliar problem representations?
- **Curriculum Learning**: Can progressive prior reduction improve discovery skills?

## Advanced Features

- **Flexible Prior Configuration**: Create custom prior levels beyond L1-L4
- **Multi-Modal Evaluation**: Both symbolic (SymPy) and LLM-based equivalence checking
- **Safe Execution**: Sandboxed environment for testing agent-generated code
- **Complete Provenance**: Full experiment history including failed hypotheses
- **Reproducible Benchmarking**: Deterministic evaluation with fixed random seeds
- **Extensible Architecture**: Easy integration of new agents and evaluation metrics

## Future Research Directions

PhysGym opens several promising avenues:

1. **Counterfactual Physics**: Create scenarios where memorized training data becomes a disadvantage, forcing models to rely on interactive reasoning
2. **Fine-Tuning for Discovery**: Investigate whether scientific reasoning skills can be learned through fine-tuning on low-prior tasks
3. **Enhanced Agent Design**:
   - Exploration modules that maximize information gain
   - Hypothesis falsification strategies
   - Automated curriculum learning using L1â†’L4 progression
4. **Knowledge Source Integration**: Identify crucial prior knowledge to develop agents that efficiently query external knowledge bases
5. **Multi-Agent Collaboration**: Coordinate specialized agents for different discovery phases

## Citation

If you use PhysGym in your research, please cite:

```bibtex
@article{chen2025physgym,
  title={PhysGym: Benchmarking LLMs in Interactive Physics Discovery with Controlled Priors},
  author={Chen, Yimeng and Pi\k{e}kos, Piotr and Ostaszewski, Mateusz and Laakom, Firas and Schmidhuber, J{\"u}rgen},
  journal={arXiv preprint arXiv:2507.15550},
  year={2025}
}
```

## Contributing

We welcome contributions! Areas of interest:

- **New baseline agents**: Implement novel discovery algorithms
- **Additional metrics**: Propose new evaluation dimensions
- **Environment extensions**: Add new physics problems or domains
- **Analysis tools**: Visualization and interpretation utilities
- **Documentation**: Tutorials, examples, and case studies

## License

This project is licensed under the MIT License.


---

**PhysGym** is designed to advance our understanding of AI scientific reasoning by providing a rigorous, controlled environment where both successes and failures illuminate the path toward truly autonomous scientific discovery agents.
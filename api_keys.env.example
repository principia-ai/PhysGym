# PhysGym LLM Configuration Example
# Copy this file to api_keys.env and fill in your configuration
#
# NOTE: Provider type (local vs remote) is auto-detected based on provider name.
#       Ollama and vLLM are recognized as local providers automatically.

# ===== LLM Provider Configuration =====
# Choose your LLM provider: ollama, vllm, openrouter, openai, anthropic, deepseek
# The system will automatically detect if it's a local or remote provider
LLM_PROVIDER=ollama

# Model configuration (depends on provider)
LLM_TEMPERATURE=0.3
LLM_MAX_TOKENS=4000
LLM_TIMEOUT=60

# ===== Local LLM Providers =====

# Ollama Configuration
OLLAMA_BASE_URL=http://localhost:11434
OLLAMA_MODEL=llama3.2

# vLLM Configuration  
VLLM_BASE_URL=http://localhost:8000
VLLM_MODEL=meta-llama/Llama-3.2-3B-Instruct
# vLLM uses HuggingFace model names

# ===== Remote API Providers =====

# OpenRouter (supports many models)
OPENROUTER_API_KEY=your_openrouter_api_key_here

# OpenAI
OPENAI_API_KEY=your_openai_api_key_here

# Anthropic Claude
ANTHROPIC_API_KEY=your_anthropic_api_key_here

# DeepSeek
DEEPSEEK_API_KEY=your_deepseek_api_key_here

# ===== Usage Examples =====
# 
# For Ollama (easiest local setup):
# 1. Install Ollama: https://ollama.ai/
# 2. Run: ollama pull llama3.2
# 3. Use: --api-provider ollama --llm-model llama3.2
#    (or set LLM_PROVIDER=ollama, LLM_MODEL=llama3.2 in this file)
#
# For vLLM (high performance):
# 1. Install vLLM: pip install vllm
# 2. Run: python -m vllm.entrypoints.openai.api_server --model meta-llama/Llama-3.2-3B-Instruct
# 3. Use: --api-provider vllm --llm-model meta-llama/Llama-3.2-3B-Instruct
#    (or set LLM_PROVIDER=vllm, LLM_MODEL=meta-llama/Llama-3.2-3B-Instruct in this file)
#
# For OpenRouter (cloud, many models):
# 1. Get API key from https://openrouter.ai/
# 2. Set OPENROUTER_API_KEY=sk-or-... in this file
# 3. Use: --api-provider openrouter --llm-model google/gemini-2.5-flash
#
# Auto-detection:
# If you don't specify --api-provider, the system automatically checks for:
# 1. Ollama (http://localhost:11434)
# 2. vLLM (http://localhost:8000)
# 3. Falls back to OpenRouter if API key is configured